{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Description"
      ],
      "metadata": {
        "id": "y9lqijZiIXKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project is designed to scrape job postings from various sources, including Indeed, LinkedIn, ZipRecruiter, Glassdoor, Google and multiple such job portals available on the internet. The application provides a streamlined interface for retrieving job data based on user-defined criteria such as search terms, location, and time of posting as well as the user being able to quickly apply to the given job. It can only be used for country of USA for now."
      ],
      "metadata": {
        "id": "oxMao46fIgHj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdSqVp9lISCu"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -U python-jobspy requests beautifulsoup4 flask python-dotenv googlemaps crochet flask_ngrok flask_cors\n",
        "!pip install -U flask\n",
        "!pip install -U pyngrok\n",
        "!pip install flask-cors\n",
        "!pip install Flask-Mail\n",
        "!pip install pyngrok\n",
        "import os\n",
        "print(os.getcwd())\n",
        "\n",
        "\n",
        "# Import necessary modules\n",
        "import csv\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import flask\n",
        "from flask import Flask, render_template, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from dotenv import load_dotenv\n",
        "import googlemaps\n",
        "from crochet import setup, wait_for\n",
        "import smtplib\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.base import MIMEBase\n",
        "from email.mime.application import MIMEApplication\n",
        "from email.utils import COMMASPACE, formatdate\n",
        "from email import encoders\n",
        "from jobspy import scrape_jobs\n",
        "from google.colab import files\n",
        "import getpass\n",
        "import os\n",
        "import threading\n",
        "from flask import Flask\n",
        "from pyngrok import ngrok, conf\n",
        "from flask_mail import Mail, Message\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "source_dir = '/content/drive/MyDrive/Python_Proj'\n",
        "destination_dir = '/content/'\n",
        "\n",
        "# Ensure the destination directory exists\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Copy the contents of the source directory to the destination\n",
        "shutil.copytree(source_dir, destination_dir, dirs_exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WebScraping APIs"
      ],
      "metadata": {
        "id": "a4nefQQFInkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "JobSpyAPI"
      ],
      "metadata": {
        "id": "Zw5iast_IrTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def JobSpyAPI(s_t,loc,h_o,c_i):\n",
        "  jobs = scrape_jobs(\n",
        "      site_name=[\"indeed\", \"linkedin\", \"zip_recruiter\", \"glassdoor\", \"google\"],\n",
        "      search_term=s_t,\n",
        "      google_search_term= s_t+\"jobs near\"+ loc+\"since\"+h_o,\n",
        "      location=loc,\n",
        "      results_wanted=10,\n",
        "      hours_old = h_o,\n",
        "      country_indeed=c_i,\n",
        "  )\n",
        "  return jobs"
      ],
      "metadata": {
        "id": "f1op2iXhIo5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenWebNinja API"
      ],
      "metadata": {
        "id": "e91RnYv5IxYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def OpenWebNinjaAPI(search_term,location,country,n):\n",
        "  import requests\n",
        "  import pandas as pd\n",
        "  import json\n",
        "\n",
        "  url = \"https://jsearch.p.rapidapi.com/search\"\n",
        "\n",
        "  querystring = {\"query\": search_term+\" jobs in\"+location,\"page\":\"1\",\"num_pages\":n,\"country\":country,\"date_posted\":\"all\"}\n",
        "\n",
        "  headers = {\n",
        "\t\"x-rapidapi-key\": \"cdad6700bdmsh5ac106aa01f9ea7p104d83jsn15813f7690aa\",\n",
        "\t\"x-rapidapi-host\": \"jsearch.p.rapidapi.com\"\n",
        "}\n",
        "\n",
        "  response = requests.get(url, headers=headers, params=querystring)\n",
        "\n",
        "  # Parse the JSON response\n",
        "  data = response.json()\n",
        "\n",
        "\n",
        "\n",
        "  # Extract the job listings from the 'data' key\n",
        "  job_listings = data.get('data', [])\n",
        "\n",
        "  # Create a DataFrame from the job listings\n",
        "  df = pd.DataFrame(job_listings)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "IaSOmiGJIk2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Maps API"
      ],
      "metadata": {
        "id": "9VFVCNy3I3-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GMapsAPI(location):\n",
        "  gmaps = googlemaps.Client(key='AIzaSyAkhVVl0ombrXrKoLIOQmhp3btAJ6alyF4')\n",
        "\n",
        "  geocode_result = gmaps.geocode(location)\n",
        "  if geocode_result:\n",
        "        lat = geocode_result[0]['geometry']['location']['lat']\n",
        "        lng = geocode_result[0]['geometry']['location']['lng']\n",
        "        return lat, lng\n",
        "  return None, None"
      ],
      "metadata": {
        "id": "vpRRUZUwI2Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning"
      ],
      "metadata": {
        "id": "cvg1lCr3I7wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def data_merger(jobspy, open_web_ninja):\n",
        "    new_columns = ['job_title', 'job_employment_type', 'employer_name', 'job_city', 'job_state', 'job_posted_at_datetime_utc', 'employer_website', 'job_apply_link']\n",
        "\n",
        "    # Process open_web_ninja data\n",
        "    new_data = open_web_ninja[new_columns]\n",
        "\n",
        "    # Process jobspy data\n",
        "    new_data_1 = pd.DataFrame()\n",
        "    new_data_1['job_title'] = jobspy['title']\n",
        "    new_data_1['job_employment_type'] = jobspy['job_type']\n",
        "    new_data_1['employer_name'] = jobspy['company']\n",
        "    new_data_1['job_city'] = jobspy['location'].apply(lambda x: x.split(',')[0].strip() if isinstance(x, str) and ',' in x else x)\n",
        "    new_data_1['job_state'] = jobspy['location'].apply(lambda x: x.split(',')[1].strip() if isinstance(x, str) and ',' in x else '')\n",
        "    new_data_1['job_posted_at_datetime_utc'] = jobspy['date_posted']\n",
        "    new_data_1['employer_website'] = jobspy['job_url_direct']\n",
        "    new_data_1['job_apply_link'] = jobspy['job_url']\n",
        "\n",
        "    # Concatenate the two DataFrames\n",
        "    combined_data = pd.concat([new_data, new_data_1], ignore_index=True)\n",
        "\n",
        "    return combined_data\n",
        "\n",
        "def data_cleaning(df):\n",
        "    # Fill missing datetime values with current date\n",
        "        df['job_posted_at_datetime_utc'].fillna('2024-12-03T00:00:00.000Z', inplace=True)\n",
        "        # Replace NaN values with None for proper JSON serialization\n",
        "        df = df.where(pd.notnull(df), None)\n",
        "        return df"
      ],
      "metadata": {
        "id": "Vo_jL-isI-4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HTMX and Flask"
      ],
      "metadata": {
        "id": "2vDZEIiIJCNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import search\n",
        "from flask import Flask, request, jsonify, render_template\n",
        "import pandas as pd\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "from flask_cors import CORS\n",
        "app = Flask(__name__, template_folder='/content/templates')\n",
        "app.config.update(\n",
        "    MAIL_SERVER='smtp.gmail.com',\n",
        "    MAIL_PORT=587,\n",
        "    MAIL_USE_TLS=True,\n",
        "    MAIL_USERNAME=\"scannjobright@gmail.com\",\n",
        "    MAIL_PASSWORD=\"czvr nwyz ophy accm\",\n",
        "    MAIL_DEFAULT_SENDER=\"scannjobright@gmail.com\",\n",
        "    TESTING=False,\n",
        "    MAIL_SUPPRESS_SEND=False\n",
        ")\n",
        "mail = Mail(app)\n",
        "\n",
        "CORS(app)\n",
        "port = 5000\n",
        "\n",
        "state='Illinois'\n",
        "location=''\n",
        "search_term=''\n",
        "empty=[]\n",
        "results_data=pd.DataFrame(empty);\n",
        "\n",
        "@app.route('/favicon.ico')\n",
        "def favicon():\n",
        "    # Provide the path to the favicon.ico file\n",
        "    return send_from_directory(\n",
        "        directory='static',  # Replace with your static folder path\n",
        "        filename='favicon.ico',\n",
        "        mimetype='image/vnd.microsoft.icon'\n",
        "    )\n",
        "\n",
        "# Global variable to cache jobs data\n",
        "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
        "def index():\n",
        "    if request.method == \"POST\":\n",
        "        try:\n",
        "            search_term = request.form.get(\"search_term\")\n",
        "            location = request.form.get(\"location\")\n",
        "            employment_type = request.form.get(\"employment_type\")\n",
        "            state = request.form.get(\"state\")\n",
        "\n",
        "            # Perform the search using the received parameters\n",
        "            jobs = JobSpyAPI(search_term, location, \"100\",\"USA\")\n",
        "            open_jobs = OpenWebNinjaAPI(search_term, location, \"USA\", \"10\")\n",
        "\n",
        "\n",
        "            results_data = data_merger(jobs, open_jobs)\n",
        "\n",
        "            if employment_type:\n",
        "                results_data = results_data[results_data['job_employment_type'] == employment_type]\n",
        "            if state:\n",
        "                results_data = results_data[results_data['job_state'] == state]\n",
        "\n",
        "            results_data = data_cleaning(results_data)\n",
        "            print(results_data)\n",
        "            results_data.to_csv(\"open_web_ninja.csv\", index=False)\n",
        "\n",
        "            return jsonify(results_data.to_dict(orient=\"records\"))\n",
        "\n",
        "            page = request.args.get('page', 1, type=int)  # Default to page 1\n",
        "            per_page = 10  # Number of items per page\n",
        "\n",
        "        except Exception as e:\n",
        "            if not all([request.form.get(\"search_term\"), request.form.get(\"location\")]):\n",
        "                return jsonify({\"error\": \"Missing required parameters\"}), 400\n",
        "\n",
        "\n",
        "    return render_template(\"index5.html\")\n",
        "\n",
        "\n",
        "\n",
        "@app.route(\"/get-jobs\", methods=['GET'])\n",
        "def get_jobs():\n",
        "    try:\n",
        "        jobs_df = pd.read_csv(\"open_web_ninja.csv\")\n",
        "        jobs_df = data_cleaning(jobs_df)\n",
        "\n",
        "        # Apply filters from query parameters\n",
        "        search_term = request.args.get(\"search_term\")\n",
        "        print(search_term)\n",
        "        location = request.args.get(\"location\")\n",
        "        print(location)\n",
        "        employment_type = request.args.get(\"employment_type\")\n",
        "        state = request.args.get(\"state\")\n",
        "\n",
        "        if search_term:\n",
        "            jobs_df = jobs_df[jobs_df['job_title'].str.contains(search_term, case=False)]\n",
        "        if location:\n",
        "            jobs_df = jobs_df[jobs_df['job_city'].str.contains(location, case=False) | jobs_df['job_state'].str.contains(location, case=False)]\n",
        "        if employment_type:\n",
        "            jobs_df = jobs_df[jobs_df['job_employment_type'] == employment_type]\n",
        "        if state:\n",
        "            jobs_df = jobs_df[jobs_df['job_state'] == state]\n",
        "        jobs_data = jobs_df.to_dict(orient=\"records\")\n",
        "        return jsonify(jobs_data)\n",
        "    except FileNotFoundError:\n",
        "        return jsonify({\"error\": \"CSV file not found\"}), 404\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "\n",
        "@app.route(\"/get-states\")\n",
        "def get_states():\n",
        "    us_states = [\n",
        "        \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\",\n",
        "        \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\",\n",
        "        \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\",\n",
        "        \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\",\n",
        "        \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\",\n",
        "        \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\",\n",
        "        \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\",\n",
        "        \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\",\n",
        "        \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\",\n",
        "        \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n",
        "    ]\n",
        "    return jsonify({'states': us_states})\n",
        "\n",
        "\n",
        "@app.route('/filter-state', methods=['GET', 'POST'])\n",
        "def filter_state():\n",
        "    state = request.form.get('state', '') if request.method == 'POST' else request.args.get('state', '')\n",
        "    try:\n",
        "        jobs_df = pd.read_csv(\"open_web_ninja.csv\")\n",
        "        jobs_df = data_cleaning(jobs_df)\n",
        "        print(jobs_df)\n",
        "        if state:\n",
        "            filtered_jobs = jobs_df[jobs_df['job_state'] == state].to_dict(orient='records')\n",
        "        else:\n",
        "            filtered_jobs = jobs_df.to_dict('records')\n",
        "        return jsonify(filtered_jobs)\n",
        "    except FileNotFoundError:\n",
        "        return jsonify({\"error\": \"No jobs found\"}), 404\n",
        "\n",
        "\n",
        "@app.route('/geocode')\n",
        "def geocode():\n",
        "    location = request.args.get('location')\n",
        "    if location:\n",
        "        lat, lng = GMapsAPI(location)\n",
        "        if lat and lng:\n",
        "            return jsonify({'lat': lat, 'lng': lng})\n",
        "    return jsonify({'error': 'Location not found'}), 404\n",
        "\n",
        "\n",
        "@app.route(\"/save-email\", methods=['POST'])\n",
        "def send_email_subscription():\n",
        "    try:\n",
        "        email = request.get_json(force=True).get('email')\n",
        "        if not email:\n",
        "            return jsonify({\"error\": \"Email is required\"}), 400\n",
        "\n",
        "        msg = Message(\n",
        "            subject=\"Job Updates Subscription\",\n",
        "            sender=\"scannjobright@gmail.com\",\n",
        "            recipients=[email],\n",
        "            body=\"Thank you for subscribing to job updates!\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            with open(\"open_web_ninja.csv\", \"rb\") as fp:\n",
        "                msg.attach(\"open_web_ninja.csv\", \"text/csv\", fp.read())\n",
        "        except FileNotFoundError:\n",
        "            app.logger.warning(\"CSV file not found\")\n",
        "\n",
        "        mail.send(msg)\n",
        "        return jsonify({\"success\": True})\n",
        "\n",
        "    except Exception as e:\n",
        "        app.logger.error(f\"Email error: {str(e)}\")\n",
        "        return jsonify({\"error\": \"Error sending email. Please try again.\"}), 500\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Install pyngrok\n",
        "    os.system('pip install pyngrok')\n",
        "\n",
        "    # Set up ngrok\n",
        "    ngrok.set_auth_token(\"2piUX4ZA1w9aqufFUk8fNF49MC7_4oJPUp4vXv8nVMhxuYWU3\")\n",
        "    public_url = ngrok.connect(port)\n",
        "    print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n",
        "\n",
        "    # Run the Flask app\n",
        "    app.run(port=port)"
      ],
      "metadata": {
        "id": "_pVZ1BHkJD_Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}