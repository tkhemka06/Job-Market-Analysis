# -*- coding: utf-8 -*-
"""Copy of Project_Python_working.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fg12TLV6hDRNkmDFe7NId8rwnmbok5g-

### Project Main

#### Required Libraries

##### DESCRIPTION
This project is designed to scrape job postings from various sources, including Indeed, LinkedIn, ZipRecruiter, Glassdoor, Google and multiple such job portals available on the internet. The application provides a streamlined interface for retrieving job data based on user-defined criteria such as search terms, location, and time of posting as well as the user being able to quickly apply to the given job. It can only  be used for country of USA for now.

##### Team Members:
Jiya Aggarwal - jagarwal@andrew.cmu.edu
Summer Lee - summerle@andrew.cmu.edu
Tanisha Khemka - tkhemka@andrew.cmu.edu
Aditya Sannabhadti - asannabh@andrew.cmu.edu
"""

# Install required packages
!pip install -U python-jobspy requests beautifulsoup4 flask python-dotenv googlemaps crochet flask_ngrok flask_cors
!pip install -U flask
!pip install -U pyngrok
!pip install flask-cors
!pip install Flask-Mail
!pip install pyngrok
import os
print(os.getcwd())


# Import necessary modules
import csv
import os
import sys
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import flask
from flask import Flask, render_template, request, jsonify
from flask_ngrok import run_with_ngrok
from dotenv import load_dotenv
import googlemaps
from crochet import setup, wait_for
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email.mime.application import MIMEApplication
from email.utils import COMMASPACE, formatdate
from email import encoders
from jobspy import scrape_jobs
from google.colab import files
import getpass
import os
import threading
from flask import Flask
from pyngrok import ngrok, conf
from flask_mail import Mail, Message

# Load environment variables
load_dotenv()

from google.colab import drive
drive.mount('/content/drive')

import shutil
import os

source_dir = '/content/drive/MyDrive/Python_Proj'
destination_dir = '/content/'

# Ensure the destination directory exists
os.makedirs(destination_dir, exist_ok=True)

# Copy the contents of the source directory to the destination
shutil.copytree(source_dir, destination_dir, dirs_exist_ok=True)

"""# WebScraping APIS
## Scraping LinkedIN, ZipRecruiter, Google, Glassdoor, Indeed

### JobSpyAPI
"""

def JobSpyAPI(s_t,loc,h_o,c_i):
  jobs = scrape_jobs(
      site_name=["indeed", "linkedin", "zip_recruiter", "glassdoor", "google"],
      search_term=s_t,
      google_search_term= s_t+"jobs near"+ loc+"since"+h_o,
      location=loc,
      results_wanted=10,
      hours_old = h_o,
      country_indeed=c_i,
  )
  return jobs

"""### OpenWebNinja API"""

def OpenWebNinjaAPI(search_term,location,country,n):
  import requests
  import pandas as pd
  import json

  url = "https://jsearch.p.rapidapi.com/search"

  querystring = {"query": search_term+" jobs in"+location,"page":"1","num_pages":n,"country":country,"date_posted":"all"}

  headers = {
	"x-rapidapi-key": "cdad6700bdmsh5ac106aa01f9ea7p104d83jsn15813f7690aa",
	"x-rapidapi-host": "jsearch.p.rapidapi.com"
}

  response = requests.get(url, headers=headers, params=querystring)

  # Parse the JSON response
  data = response.json()



  # Extract the job listings from the 'data' key
  job_listings = data.get('data', [])

  # Create a DataFrame from the job listings
  df = pd.DataFrame(job_listings)

  return df

"""### Google Maps API
For getting the heatmap of where the jobs are located
"""

def GMapsAPI(location):
  gmaps = googlemaps.Client(key='AIzaSyAkhVVl0ombrXrKoLIOQmhp3btAJ6alyF4')

  geocode_result = gmaps.geocode(location)
  if geocode_result:
        lat = geocode_result[0]['geometry']['location']['lat']
        lng = geocode_result[0]['geometry']['location']['lng']
        return lat, lng
  return None, None

"""### Data Cleaning
For cleaning up nulls in the dataset
"""

import pandas as pd

def data_merger(jobspy, open_web_ninja):
    new_columns = ['job_title', 'job_employment_type', 'employer_name', 'job_city', 'job_state', 'job_posted_at_datetime_utc', 'employer_website', 'job_apply_link']

    # Process open_web_ninja data
    new_data = open_web_ninja[new_columns]

    # Process jobspy data
    new_data_1 = pd.DataFrame()
    new_data_1['job_title'] = jobspy['title']
    new_data_1['job_employment_type'] = jobspy['job_type']
    new_data_1['employer_name'] = jobspy['company']
    new_data_1['job_city'] = jobspy['location'].apply(lambda x: x.split(',')[0].strip() if isinstance(x, str) and ',' in x else x)
    new_data_1['job_state'] = jobspy['location'].apply(lambda x: x.split(',')[1].strip() if isinstance(x, str) and ',' in x else '')
    new_data_1['job_posted_at_datetime_utc'] = jobspy['date_posted']
    new_data_1['employer_website'] = jobspy['job_url_direct']
    new_data_1['job_apply_link'] = jobspy['job_url']

    # Concatenate the two DataFrames
    combined_data = pd.concat([new_data, new_data_1], ignore_index=True)

    return combined_data

def data_cleaning(df):
    # Fill missing datetime values with current date
        df['job_posted_at_datetime_utc'].fillna('2024-12-03T00:00:00.000Z', inplace=True)
        # Replace NaN values with None for proper JSON serialization
        df = df.where(pd.notnull(df), None)
        return df

"""### HTMX and Flask
Functions related to communication between the server
"""

from re import search
from flask import Flask, request, jsonify, render_template
import pandas as pd
from pyngrok import ngrok
import os
from flask_cors import CORS
app = Flask(__name__, template_folder='/content/templates')
app.config.update(
    MAIL_SERVER='smtp.gmail.com',
    MAIL_PORT=587,
    MAIL_USE_TLS=True,
    MAIL_USERNAME="scannjobright@gmail.com",
    MAIL_PASSWORD="czvr nwyz ophy accm",
    MAIL_DEFAULT_SENDER="scannjobright@gmail.com",
    TESTING=False,
    MAIL_SUPPRESS_SEND=False
)
mail = Mail(app)

CORS(app)
port = 5000

state='Illinois'
location=''
search_term=''
empty=[]
results_data=pd.DataFrame(empty);

@app.route('/favicon.ico')
def favicon():
    # Provide the path to the favicon.ico file
    return send_from_directory(
        directory='static',  # Replace with your static folder path
        filename='favicon.ico',
        mimetype='image/vnd.microsoft.icon'
    )

# Global variable to cache jobs data
@app.route("/", methods=["GET", "POST"])
def index():
    if request.method == "POST":
        try:
            search_term = request.form.get("search_term")
            location = request.form.get("location")
            employment_type = request.form.get("employment_type")
            state = request.form.get("state")

            # Perform the search using the received parameters
            jobs = JobSpyAPI(search_term, location, "100","USA")
            open_jobs = OpenWebNinjaAPI(search_term, location, "USA", "10")


            results_data = data_merger(jobs, open_jobs)

            if employment_type:
                results_data = results_data[results_data['job_employment_type'] == employment_type]
            if state:
                results_data = results_data[results_data['job_state'] == state]

            results_data = data_cleaning(results_data)
            print(results_data)
            results_data.to_csv("open_web_ninja.csv", index=False)

            return jsonify(results_data.to_dict(orient="records"))

            page = request.args.get('page', 1, type=int)  # Default to page 1
            per_page = 10  # Number of items per page

        except Exception as e:
            if not all([request.form.get("search_term"), request.form.get("location")]):
                return jsonify({"error": "Missing required parameters"}), 400


    return render_template("index5.html")



@app.route("/get-jobs", methods=['GET'])
def get_jobs():
    try:
        jobs_df = pd.read_csv("open_web_ninja.csv")
        jobs_df = data_cleaning(jobs_df)

        # Apply filters from query parameters
        search_term = request.args.get("search_term")
        print(search_term)
        location = request.args.get("location")
        print(location)
        employment_type = request.args.get("employment_type")
        state = request.args.get("state")

        if search_term:
            jobs_df = jobs_df[jobs_df['job_title'].str.contains(search_term, case=False)]
        if location:
            jobs_df = jobs_df[jobs_df['job_city'].str.contains(location, case=False) | jobs_df['job_state'].str.contains(location, case=False)]
        if employment_type:
            jobs_df = jobs_df[jobs_df['job_employment_type'] == employment_type]
        if state:
            jobs_df = jobs_df[jobs_df['job_state'] == state]
        jobs_data = jobs_df.to_dict(orient="records")
        return jsonify(jobs_data)
    except FileNotFoundError:
        return jsonify({"error": "CSV file not found"}), 404
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/get-states")
def get_states():
    us_states = [
        "Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado",
        "Connecticut", "Delaware", "Florida", "Georgia", "Hawaii", "Idaho",
        "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana",
        "Maine", "Maryland", "Massachusetts", "Michigan", "Minnesota",
        "Mississippi", "Missouri", "Montana", "Nebraska", "Nevada",
        "New Hampshire", "New Jersey", "New Mexico", "New York",
        "North Carolina", "North Dakota", "Ohio", "Oklahoma", "Oregon",
        "Pennsylvania", "Rhode Island", "South Carolina", "South Dakota",
        "Tennessee", "Texas", "Utah", "Vermont", "Virginia", "Washington",
        "West Virginia", "Wisconsin", "Wyoming"
    ]
    return jsonify({'states': us_states})


@app.route('/filter-state', methods=['GET', 'POST'])
def filter_state():
    state = request.form.get('state', '') if request.method == 'POST' else request.args.get('state', '')
    try:
        jobs_df = pd.read_csv("open_web_ninja.csv")
        jobs_df = data_cleaning(jobs_df)
        print(jobs_df)
        if state:
            filtered_jobs = jobs_df[jobs_df['job_state'] == state].to_dict(orient='records')
        else:
            filtered_jobs = jobs_df.to_dict('records')
        return jsonify(filtered_jobs)
    except FileNotFoundError:
        return jsonify({"error": "No jobs found"}), 404


@app.route('/geocode')
def geocode():
    location = request.args.get('location')
    if location:
        lat, lng = GMapsAPI(location)
        if lat and lng:
            return jsonify({'lat': lat, 'lng': lng})
    return jsonify({'error': 'Location not found'}), 404


@app.route("/save-email", methods=['POST'])
def send_email_subscription():
    try:
        email = request.get_json(force=True).get('email')
        if not email:
            return jsonify({"error": "Email is required"}), 400

        msg = Message(
            subject="Job Updates Subscription",
            sender="scannjobright@gmail.com",
            recipients=[email],
            body="Thank you for subscribing to job updates!"
        )

        try:
            with open("open_web_ninja.csv", "rb") as fp:
                msg.attach("open_web_ninja.csv", "text/csv", fp.read())
        except FileNotFoundError:
            app.logger.warning("CSV file not found")

        mail.send(msg)
        return jsonify({"success": True})

    except Exception as e:
        app.logger.error(f"Email error: {str(e)}")
        return jsonify({"error": "Error sending email. Please try again."}), 500


if __name__ == "__main__":
    # Install pyngrok
    os.system('pip install pyngrok')

    # Set up ngrok
    ngrok.set_auth_token("2piUX4ZA1w9aqufFUk8fNF49MC7_4oJPUp4vXv8nVMhxuYWU3")
    public_url = ngrok.connect(port)
    print(f" * ngrok tunnel \"{public_url}\" -> \"http://127.0.0.1:{port}\"")

    # Run the Flask app
    app.run(port=port)